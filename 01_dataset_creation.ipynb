{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"29CGggg2XQ7-"},"outputs":[],"source":["datapath = \"./dataset/MassiveSumm/\"\n","column_order = ['text', 'label', 'length', 'source', 'language', 'domain', 'topic']\n","selected = ['ar', 'ca', 'cs', 'de', 'en', 'es', 'nl', 'pt', 'ru', 'uk', 'zh']\n","\n","#export datapath to be accesible by a bash script\n","with open('datapath.txt', 'w') as f:\n","    f.write(datapath)"]},{"cell_type":"code","source":["!mkdir -p $(cat datapath.txt)"],"metadata":{"id":"vDb5vuDKNKhD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HMvSnoRDXSoj"},"outputs":[],"source":["#mount GDrive if datapath is on the drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"7pbJVlXir8CX"},"source":["# Download Data\n","\n","\n","*   Problem with wayback, only Common Crawl (cc) links and MassiveSumm-full (author version) used\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pGRgjU7VWjhF"},"outputs":[],"source":["!git clone https://github.com/danielvarab/massive-summ.git\n","!pip install -r massive-summ/requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lLr_sy1CWj7X"},"outputs":[],"source":["import gdown\n","import pandas as pd\n","import glob\n","\n","urls = pd.read_csv('massive-summ/urls.tsv', sep='\\t')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GhSjCX3UYPuN"},"outputs":[],"source":["urls"]},{"cell_type":"markdown","metadata":{"id":"YelspfIzMpIO"},"source":["##Common Crawl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O7IWb7ynMz1r"},"outputs":[],"source":["url_list = []\n","for l in urls['cc']:\n","  if l is None or l == \"-\": continue\n","  url_list.append(l.replace('/view)','').split('/')[-1])\n","for file_id in url_list:\n","  url = f'https://drive.google.com/uc?id={file_id}'\n","  gdown.download(url, quiet=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6JPp14z3N6bz"},"outputs":[],"source":["%%bash\n","\n","for f in $(ls massive-in-cc.*.jsonl.gz); do\n","  python massive-summ/scripts/download.py --urls $f --archive temp --n_proc 4;\n","  python massive-summ/scripts/extract.py --archive temp --dataset ${f#*.};\n","  cp ${f#*.} $(cat datapath.txt)\n","  rm $f\n","  #break\n","done"]},{"cell_type":"markdown","metadata":{"id":"q3elxDVUMu5d"},"source":["## WayBack"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3m-_mgHgP0T"},"outputs":[],"source":["!git clone https://github.com/danielvarab/da-newsroom.git\n","!pip install -r da-newsroom/requirements.txt\n","!pip install -e ./da-newsroom/newsroom-lib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"riEO-LL0Wj-i"},"outputs":[],"source":["url_list = []\n","for l in urls['wayback']:\n","  if l is None or l == \"-\": continue\n","  url_list.append(l.replace('/view)','').split('/')[-1])\n","for file_id in url_list:\n","  url = f'https://drive.google.com/uc?id={file_id}'\n","  gdown.download(url, quiet=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SihjziX8OY-e"},"outputs":[],"source":["%%bash\n","\n","for f in $(ls -1 *.jsonl.gz | grep -v \"wb-\\|-wb\"); do\n","  mv $f \"massive-in-wb.$f\";\n","done"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KYZmqUBDX-Od"},"outputs":[],"source":["%%bash\n","\n","for f in $(ls -1 massive-in-wb.*.jsonl.gz); do\n","  python massive-summ/scripts/download.py --urls $f --archive temp --n_proc 4;\n","  #python massive-summ/scripts/extract.py --archive temp --dataset wb-${f#*.};\n","  #cp wb-${f#*.} $(cat datapath.txt)\n","  #rm $f\n","  #break\n","done"]},{"cell_type":"markdown","metadata":{"id":"S8I-Pf8XPG3E"},"source":["## MassiveSumm-full\n","\n","\n","*   direct links to dataset per language received from authors\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kjzygMstJD3n"},"outputs":[],"source":["urls = pd.read_csv(datapath + 'massive-summ by language (links) - massive-summ-full.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_s7P1VIJDYb_"},"outputs":[],"source":["processed = []\n","for f in glob.glob(datapath + '*.all.jsonl.gz'):\n","  processed.append(f.split('/')[-1].split('.')[0])\n","temp = [x not in processed for x in urls.alpha_3]\n","urls = urls[temp]\n","urls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5jyAhesfLqbb"},"outputs":[],"source":["url_list = []\n","for l in urls['link']:\n","  if l is None or l is pd.NA or l == \"-\": continue\n","  url_list.append(str(l).replace('/view','').split('/')[-1])\n","for file_id in url_list:\n","  url = f'https://drive.google.com/uc?id={file_id}'\n","  gdown.download(url, quiet=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qqykVHBrWibc"},"outputs":[],"source":["%%bash\n","\n","for f in $(ls *.all.jsonl.gz); do\n","  cp ${f} $(cat datapath.txt)\n","  rm $f\n","  #break\n","done"]},{"cell_type":"markdown","metadata":{"id":"dozsOzyyshBd"},"source":["# Preprocessing (Clean & Clear) Human-Text Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fOKFQivngbEY"},"outputs":[],"source":["!sudo apt-get install libicu-dev > /dev/null\n","!pip install polyglot PyICU pycld2 morfessor > /dev/null"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ce5AmqotliEZ"},"outputs":[],"source":["#[1] A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, Bag of Tricks for Efficient Text Classification\n","#[2] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. Jégou, T. Mikolov, FastText.zip: Compressing text classification models\n","!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin -O lid.176.bin > /dev/null\n","!pip install fasttext > /dev/null"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BxKB5gd6XD4f"},"outputs":[],"source":["import pandas as pd\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)\n","import glob\n","import shutil\n","from langcodes import *\n","import polyglot\n","from polyglot.text import Text, Word\n","from polyglot.detect.base import logger as polyglot_logger\n","polyglot_logger.setLevel(\"ERROR\")\n","import fasttext\n","fasttext_model = fasttext.load_model('lid.176.bin')\n","from collections import Counter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCfdka4dar7R"},"outputs":[],"source":["filelist = glob.glob(datapath + '*.jsonl.gz')"]},{"cell_type":"code","source":["selected_filelist = []\n","for f in filelist:\n","  if standardize_tag(f.split('/')[-1].split('.')[0]) in selected:\n","    selected_filelist.append(f)\n","filelist = selected_filelist"],"metadata":{"id":"Ws_JsRYvTQbN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tcyc1CC6a1lE"},"outputs":[],"source":["%%time\n","\n","def clear_dataset(df):\n","  df_string_columns = df.select_dtypes(['object'])\n","  df[df_string_columns.columns] = df_string_columns.apply(lambda x: x.str.strip())\n","  return df\n","\n","def try_to_detect(text):\n","  fasttext_pred, fasttext_prob, polyglot_pred, polyglot_prob = 0, 0, 0, 0\n","  try:\n","    pred, prob = fasttext_model.predict(text.split('\\n')[0])\n","    #pred, prob = fasttext_model.predict(text.replace('\\n', ' '))\n","    fasttext_pred = pred[0].replace('__label__', '')\n","    fasttext_prob = prob[0]\n","    language = Text(text).language\n","    polyglot_pred = language.code\n","    polyglot_prob = language.confidence\n","    return fasttext_pred, fasttext_prob, polyglot_pred, polyglot_prob\n","  except:\n","    pass\n","  return fasttext_pred, fasttext_prob, polyglot_pred, polyglot_prob\n","\n","def try_to_detect_language(title, text):\n","  title_fasttext_pred, title_fasttext_prob, title_polyglot_pred, title_polyglot_prob = try_to_detect(title)\n","  fasttext_pred, fasttext_prob, polyglot_pred, polyglot_prob = try_to_detect(text)\n","\n","  detected_language = \"unknown\"\n","\n","  if title_fasttext_prob < 0.9: title_fasttext_pred = \"unknown\"\n","  if title_polyglot_prob < 0.9: title_polyglot_pred = \"unknown\"\n","  if fasttext_prob < 0.9: fasttext_pred = \"unknown\"\n","  if polyglot_prob < 0.9: polyglot_pred = \"unknown\"\n","\n","  c = Counter([title_fasttext_pred, title_polyglot_pred, fasttext_pred, polyglot_pred])\n","  if c.most_common()[0][1] > 2:\n","    detected_language = c.most_common()[0][0]\n","\n","  return detected_language\n","\n","stat = pd.DataFrame(columns=['language', 'size', 'note'])\n","massivesumm = pd.DataFrame()\n","for f in filelist:\n","  #if (\"wb-\" in f) or (\".all.\" in f): continue\n","  df = pd.DataFrame()\n","  temp = pd.read_json(f, lines=True, chunksize=10000, nrows=50000) #up to 50k samples per file taken\n","  for chunk in temp:\n","    df = pd.concat([df, chunk.astype(\"string\")], copy=False)\n","  df.drop(columns=['date', 'summary'], inplace=True)\n","  if (\".all.\" not in f):\n","    df.drop(columns=['archive'], inplace=True)\n","    language_source = standardize_tag(f.split('/')[-1].replace('.jsonl.gz', ''))\n","  else:\n","    language_source = standardize_tag(f.split('/')[-1].replace('.all.jsonl.gz', ''))\n","  df['language'] = language_source\n","  stat.loc[len(stat.index)] = [language_source, len(df), 'original']\n","  df = clear_dataset(df)\n","  df.replace('', pd.NA, inplace=True)\n","  df.dropna(inplace=True)\n","  df.drop_duplicates(inplace=True)\n","  #df.drop_duplicates(subset=['text'], inplace=True)\n","  #df.drop_duplicates(subset=['title'], inplace=True)\n","  stat.loc[len(stat.index)] = [language_source, len(df), 'NA_dup_removed']\n","  df['temp'] = df.title.str.split().apply(len)\n","  df = df[df.temp > 1]\n","  df = df.drop(columns=['temp'])\n","  df['temp'] = [len(x.split()) for x in df.text] #df.text.str.split().apply(len)\n","  df = df[df.temp > 5]#.copy()\n","  df = df.drop(columns=['temp'])\n","  stat.loc[len(stat.index)] = [language_source, len(df), 'min_textsize_applied']\n","  df['detected_language'] = df.apply(lambda x: try_to_detect_language(x['title'], x['text']), axis = 1)\n","  df = df[(df.language == df.detected_language)]\n","  df = df.drop(columns=['detected_language'])\n","  stat.loc[len(stat.index)] = [language_source, len(df), 'language_checked']\n","  df = df.sample(min(5000, len(df)), random_state = 0).sample(frac=1., random_state = 0).reset_index(drop=True)\n","  massivesumm = pd.concat([massivesumm, df], ignore_index=True, copy=False)\n","  #break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jIHb5IQWU2A4"},"outputs":[],"source":["original = stat[stat.note == 'original'].groupby(by=['language']).sum(numeric_only=True)['size']\n","NA_dup_removed = stat[stat.note == 'NA_dup_removed'].groupby(by=['language']).sum(numeric_only=True)['size']\n","min_textsize_applied = stat[stat.note == 'min_textsize_applied'].groupby(by=['language']).sum(numeric_only=True)['size']\n","language_checked = stat[stat.note == 'language_checked'].groupby(by=['language']).sum(numeric_only=True)['size']\n","temp = pd.concat([original, NA_dup_removed, min_textsize_applied, language_checked], axis=1)\n","temp.columns = ['original', 'NA_dup_removed', 'min_textsize_applied', 'language_checked']\n","temp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9h-7KIV1Zhwz"},"outputs":[],"source":["total = stat.groupby('note').sum(numeric_only=True)\n","total"]},{"cell_type":"code","source":["total = total.T\n","total = total.rename(columns={'note':'language'}, index={'size':'Total'})\n","total.columns.name = 'language'\n","total"],"metadata":{"id":"Aeq4oCj_7eEb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["temp = temp.append(total).reset_index()\n","print(temp.to_latex(index=False, na_rep=0, escape=False, formatters={\"text\": str.lower}, float_format=\"{:.0f}\".format))"],"metadata":{"id":"CXb94CD1EniD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kz8MTQnKc3fW"},"outputs":[],"source":["massivesumm.language.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abNsVib-q3qZ"},"outputs":[],"source":["#remove dulicates (among files - e.g. from CC and all)\n","massivesumm.drop_duplicates(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KTG9eq0lLhtC"},"outputs":[],"source":["massivesumm.language.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5kCVeoT1MKUn"},"outputs":[],"source":["massivesumm.to_csv(datapath + 'MassiveSumm_selected.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rIzFR8u4he_I"},"outputs":[],"source":["# remove news venue from text (e.g. CNN, BBC)\n","#massivesumm['temp'] = massivesumm['url'].str.replace('https://', '', regex=False).str.replace('http://', '', regex=False).str.replace('www.', '', regex=False).apply(lambda x: x.split('/')[0].lower())\n","#massivesumm['temp'] = massivesumm['temp'].str.replace('.com.', '.', regex=False).str.replace('.co.', '.', regex=False).str.replace('.org.', '.', regex=False).str.replace('.net.', '.', regex=False)\n","#massivesumm['temp'] = massivesumm['temp'].apply(lambda x: x.split('.')[-2])\n","#massivesumm['text'] = [x.replace(str(y), '').replace(str(y).upper(), '').replace('  ', ' ') for x, y  in massivesumm[['text','temp']].to_numpy()]\n","#massivesumm.drop(columns=['temp'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOCJ0HRgWwlg"},"outputs":[],"source":["#massivesumm.to_csv(datapath + 'MassiveSumm_removed_platform_from_text.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"HxOn9uQmt3r0"},"source":["# Convert to Unified Form"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Bt6NK0bR2Yp"},"outputs":[],"source":["import pandas as pd\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ao_W7LpZRjrC"},"outputs":[],"source":["massivesumm = pd.read_csv(datapath + 'MassiveSumm_selected.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FoUvMcA1SQcB"},"outputs":[],"source":["massivesumm = massivesumm.astype('string')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sxu2iXkNSEtV"},"outputs":[],"source":["massivesumm.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JqezdhNQt-TX"},"outputs":[],"source":["%%time\n","massivesumm['label'] = 'human'\n","#ToDo: list() for Chinese to obtain letters or use some NLP library to get words\n","massivesumm['length'] = [len(x.split()) for x in massivesumm.text] #massivesumm['text'].str.split().apply(len)\n","massivesumm['temp'] = massivesumm['url'].str.replace('https://', '', regex=False).str.replace('http://', '', regex=False).str.replace('www.', '', regex=False).apply(lambda x: x.split('/')[0].lower())\n","massivesumm['temp'] = massivesumm['temp'].str.replace('.com.', '.', regex=False).str.replace('.co.', '.', regex=False).str.replace('.org.', '.', regex=False).str.replace('.net.', '.', regex=False)\n","massivesumm['temp'] = massivesumm['temp'].apply(lambda x: x.split('.')[-2])\n","massivesumm['source'] = 'MassiveSumm_' + massivesumm['temp']\n","massivesumm.drop(columns=['temp'], inplace=True)\n","massivesumm['domain'] = 'news'\n","massivesumm['topic'] = 'unknown'\n","\n","massivesumm = massivesumm[massivesumm.length > 5].reset_index(drop=True)\n","\n","massivesumm['keep'] = False\n","massivesumm.loc[massivesumm.index.isin(massivesumm[massivesumm.language == 'en'].sample(3300, random_state = 0).index), 'keep'] = True\n","massivesumm.loc[massivesumm.index.isin(massivesumm[massivesumm.language == 'es'].sample(1300, random_state = 0).index), 'keep'] = True\n","massivesumm.loc[massivesumm.index.isin(massivesumm[massivesumm.language == 'ru'].sample(1300, random_state = 0).index), 'keep'] = True\n","massivesumm.loc[massivesumm.index.isin(massivesumm[massivesumm.language == 'ar'].sample(300, random_state = 0).index), 'keep'] = True\n","massivesumm.loc[massivesumm.index.isin(massivesumm[massivesumm.language == 'ca'].sample(300, random_state = 0).index), 'keep'] = True\n","massivesumm.loc[massivesumm.index.isin(massivesumm[massivesumm.language == 'cs'].sample(300, random_state = 0).index), 'keep'] = True\n","massivesumm.loc[massivesumm.index.isin(massivesumm[massivesumm.language == 'de'].sample(300, random_state = 0).index), 'keep'] = True\n","massivesumm.loc[massivesumm.index.isin(massivesumm[massivesumm.language == 'nl'].sample(300, random_state = 0).index), 'keep'] = True\n","massivesumm.loc[massivesumm.index.isin(massivesumm[massivesumm.language == 'pt'].sample(300, random_state = 0).index), 'keep'] = True\n","massivesumm.loc[massivesumm.index.isin(massivesumm[massivesumm.language == 'uk'].sample(300, random_state = 0).index), 'keep'] = True\n","massivesumm.loc[massivesumm.index.isin(massivesumm[massivesumm.language == 'zh'].sample(300, random_state = 0).index), 'keep'] = True\n","\n","#remaining data\n","massivesumm_rem = massivesumm.loc[~massivesumm.keep]\n","massivesumm_rem.drop_duplicates(subset=['text'], inplace=True)\n","massivesumm_rem.drop_duplicates(subset=['title'], inplace=True)\n","massivesumm_rem = massivesumm_rem.drop(columns=['keep'])\n","massivesumm_rem = massivesumm_rem.sample(frac=1., random_state = 0).reset_index(drop=True)\n","massivesumm_rem['split'] = \"train\"\n","massivesumm_rem.to_csv(datapath + 'MassiveSumm_selected2.csv', index=False)\n","\n","massivesumm = massivesumm.loc[massivesumm.keep]\n","massivesumm = massivesumm.drop(columns=['keep'])\n","massivesumm = massivesumm.sample(frac=1., random_state = 0).reset_index(drop=True)\n","\n","massivesumm['split'] = \"train\"\n","test_split = massivesumm.groupby(['language']).sample(300, random_state = 0)\n","massivesumm.loc[massivesumm.index.isin(test_split.index), 'split'] = \"test\"\n","\n","#massivesumm.drop(columns=['url', 'title'], inplace=True)\n","#massivesumm = massivesumm[column_order]\n","massivesumm.to_csv(datapath + 'MassiveSumm_selected.csv', index=False)"]},{"cell_type":"code","source":["print(massivesumm[massivesumm.split == \"train\"].language.value_counts())\n","print(massivesumm[massivesumm.split == \"test\"].language.value_counts())"],"metadata":{"id":"Bj6hQQYATRKB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k55fryAkBJg5"},"outputs":[],"source":["massivesumm.language.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9zawAi-QBNga"},"outputs":[],"source":["massivesumm.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THrMAkQOCwCc"},"outputs":[],"source":["massivesumm.source.value_counts().reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jDv5a0XmC6MY"},"outputs":[],"source":["massivesumm.drop_duplicates(keep=False, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DaQzuo7DEWg2"},"outputs":[],"source":["massivesumm.language.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PQMEP3tVEZa1"},"outputs":[],"source":["len(massivesumm.language.unique())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDPPpP8GI3cp"},"outputs":[],"source":["str(sorted(massivesumm.language.unique()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKFDo0Naq9El"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# Check New Samples"],"metadata":{"id":"EZDAEgtE-He7"}},{"cell_type":"code","source":["from tqdm import tqdm"],"metadata":{"id":"eQnbcgJugJTt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(datapath + f'MassiveSumm_selected.csv')\n","print(df[df.duplicated(['title'], keep='first')].language.value_counts())\n","print(df[df.duplicated(['text'], keep='first')].language.value_counts())\n","print(df[df.duplicated(['title','text'], keep='first')].language.value_counts())"],"metadata":{"id":"IjdhLEiCPeaM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df2 = pd.read_csv(datapath + f'MassiveSumm_selected2.csv')\n","df2[df2.duplicated(['title'], keep=False)].language.value_counts()"],"metadata":{"id":"YPCOKLRO-guz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["present = []\n","for index, row in tqdm(df2.iterrows()):\n","  temp = False\n","  if row.title in df.title.to_list():\n","    temp = True\n","  present.append(temp)\n","df2['present'] = present"],"metadata":{"id":"pU5a3Y48-n7P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df2.present.value_counts()"],"metadata":{"id":"cC7RudSN_XoS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["to_be_changed = df[df.duplicated(['text']) | df.duplicated(['title'])]\n","df = df[~df.duplicated(['text']) & ~df.duplicated(['title'])]\n","to_be_changed.language.value_counts()"],"metadata":{"id":"L73uIrlZFZQE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_data = df2[~df2.present]\n","new_data['keep'] = False\n","new_data.loc[new_data.index.isin(new_data[new_data.language == 'en'].sample(215, random_state = 0).index), 'keep'] = True\n","new_data.loc[new_data.index.isin(new_data[new_data.language == 'es'].sample(79, random_state = 0).index), 'keep'] = True\n","new_data.loc[new_data.index.isin(new_data[new_data.language == 'pt'].sample(15, random_state = 0).index), 'keep'] = True\n","new_data.loc[new_data.index.isin(new_data[new_data.language == 'de'].sample(11, random_state = 0).index), 'keep'] = True\n","new_data.loc[new_data.index.isin(new_data[new_data.language == 'ar'].sample(1, random_state = 0).index), 'keep'] = True\n","new_data.loc[new_data.index.isin(new_data[new_data.language == 'zh'].sample(1, random_state = 0).index), 'keep'] = True\n","new_data = new_data.loc[new_data.keep]\n","new_data = new_data.drop(columns=['keep'])\n","new_data = new_data.sample(frac=1., random_state = 0).reset_index(drop=True)\n","new_data.language.value_counts()"],"metadata":{"id":"gOaCTPc0Gqa0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_data.present.value_counts()"],"metadata":{"id":"0F-LzXSfEIgQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocess Generated Data\n","\n","After machine-texts generation from LLM based on human-texts obtained from above"],"metadata":{"id":"2q7GoG7OPfFE"}},{"cell_type":"code","source":["#[1] A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, Bag of Tricks for Efficient Text Classification\n","#[2] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. Jégou, T. Mikolov, FastText.zip: Compressing text classification models\n","#!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin -O lid.176.bin > /dev/null\n","#!pip install fasttext > /dev/null\n","#import fasttext\n","#fasttext_model = fasttext.load_model('lid.176.bin')"],"metadata":{"id":"UdlVLAddRCoh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install fasttext-langdetect language_data > /dev/null\n","#!python -m spacy download zh_core_web_sm > /dev/null\n","!pip install -U git+https://github.com/aboSamoor/polyglot.git@master --quiet"],"metadata":{"id":"uOj2UihJTITl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from ftlangdetect import detect\n","from tqdm import tqdm\n","from collections import Counter\n","from langcodes import *\n","from polyglot.text import Text, Word\n","import regex\n","#import spacy\n","#nlp_zh = spacy.load('zh_core_web_sm')\n","pd.set_option('display.max_rows', 100)\n","tqdm.pandas()"],"metadata":{"id":"M7EgH8WzPjW-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models = ['text-davinci-003', 'gpt-3.5-turbo', 'gpt-4', 'alpaca-lora-30b', 'vicuna-13b', 'llama-65b', 'opt-66b', 'opt-iml-max-1.3b']\n","datasets = {}\n","\n","multitude = pd.read_csv(datapath + f'MassiveSumm_selected.csv')\n","\n","for model in models:\n","  temp = pd.read_csv(datapath + f'MassiveSumm_selected_{model}.csv')\n","  datasets[model] = temp"],"metadata":{"id":"SyG15ZZixvth"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#213 duplicated titles, 310 duplicated texts, 3 titles could be in both splits\n","multitude[multitude.duplicated(['title'], keep=False)].groupby(['title']).split.value_counts()"],"metadata":{"id":"aUyvlCmRqfkG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#remove whitespaces around texts\n","def clear_dataset(df):\n","  df_string_columns = df.select_dtypes(['object'])\n","  df[df_string_columns.columns] = df_string_columns.apply(lambda x: x.str.strip())\n","  return df\n","\n","#remove some unicode chars making problems in polyglot\n","#https://github.com/aboSamoor/polyglot/issues/71#issuecomment-707997790\n","def remove_bad_chars(text):\n","  RE_BAD_CHARS = regex.compile(r\"[\\p{Cc}\\p{Cs}]+\")\n","  return RE_BAD_CHARS.sub(\"\", text)\n","\n","#remove prompts from generated text\n","def remove_prompts(row):\n","  language = row.language\n","  language_name = Language.make(language=row.language).display_name()\n","  headline = row.title\n","  prompt = f'You are a multilingual journalist.\\n\\nTask: Write a news article in {language_name} for the following headline: \"{headline}\". Leave out the instructions, return just the text of the article.\\n\\nOutput:'\n","  #alpaca prompt\n","  prompt2 = f'<unk>### Instruction:\\nYou are a multilingual journalist.\\n\\nTask: Write a news article in {language_name} for the following headline: \"{headline}\". Leave out the instructions, return just the text of the article.\\n\\n\\n\\n### Response:'\n","  text = str(row.generated).strip()\n","  text = text.replace(prompt2, '').strip()\n","  text = text.replace(''.join(prompt2.split()), '').strip()\n","  text = text.replace(prompt, '').strip()\n","  text = text.replace(''.join(prompt.split()), '').strip()\n","  text = text.replace(f'\"{row.title}\"', '').strip()\n","  text = text.replace(row.title, '').strip()\n","  return text\n","  text = text.replace('###', '').strip()\n","  text = text.replace('Instruction:', '').strip()\n","  text = text.replace('You are a multilingual journalist.', '').strip()\n","  text = text.replace('Task:', '').strip()\n","  text = text.replace(f'Write a news article in {language_name} for the following headline:', '').strip()\n","  text = text.replace('\\\"\\\".', '').strip()\n","  text = text.replace('Leave out the instructions, return just the text of the article.', '').strip()\n","  text = text.replace('Response:', '').strip()\n","  return text\n","\n","#remove unfinished final sentence from generated text\n","def remove_unended_sentence(row):\n","  text = Text(row.generated, hint_language_code=row.language)\n","  if (row.generated != '') and (len(text.sentences) > 1):\n","    if (text.sentences[-1].words[-1] not in ['。', '؟', '!', '?', '.']): #final sentence not ended by any of these characters\n","      return row.generated.removesuffix(str(text.sentences[-1]))\n","  return row.generated\n","\n","#detect language of generated text\n","def fasttext_detect_language(dataset):\n","  generated_languages = []\n","  for index, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n","    if str(row.generated) != \"nan\":\n","      generated_languages.append(detect(text=row.generated.replace('\\n', ' '), low_memory=False)['lang'])\n","    else:\n","      generated_languages.append(row.language)\n","  return generated_languages\n","\n","#shorten generated texts\n","def shorten_generated(row):\n","  generated = str(row.generated).strip()\n","  if (generated == ''):\n","    return generated\n","  generated_length = len(row.generated.split())\n","  if (row.language == 'zh'):\n","    generated_length = len(Text(row.generated, hint_language_code=row.language).words)\n","  human_length = len(row.text.split())\n","  if (row.language == 'zh'):\n","    human_length = len(Text(row.text, hint_language_code=row.language).words)\n","\n","  if (human_length == 0):\n","    return generated\n","\n","  while (human_length < (generated_length - 5)): #remove last sentence while more than 5 words longer\n","    text = Text(generated, hint_language_code=row.language)\n","    if (len(text.sentences) < 2): #single sentence will not be removed\n","      return generated\n","    generated = generated.removesuffix(str(text.sentences[-1])).strip()\n","    generated_length = len(generated.split())\n","    if (row.language == 'zh'):\n","      generated_length = len(Text(generated, hint_language_code=row.language).words)\n","  return generated\n","\n","#unify dataset form\n","def unify_form(dataset, model):\n","  dataset = clear_dataset(dataset)\n","  dataset['label'] = model\n","  dataset['text'] = dataset['generated']\n","  dataset['length'] = [len(x.split()) if (y != 'zh') or (x == '') else len(Text(x, hint_language_code=y).words) for (x, y) in zip(dataset.text, dataset.language)]\n","  dataset['source'] = [f'MULTITuDE_{x}' for x in dataset.source]\n","  return dataset\n","\n","#uniqueness/repetitiveness - get number of unique sentences in row.text\n","def unique_sentences(row):\n","  if row.text == '':\n","    return 0\n","  sentences = Text(row.text, hint_language_code=row.language).sentences\n","  return len(set(sentences)) / len(sentences)\n","\n","#uniqueness/repetitiveness - get number of unique words in row.text\n","def unique_words(row):\n","  if row.text == '':\n","    return 0\n","  words = Text(row.text, hint_language_code=row.language).words\n","  return len(set(words)) / len(words)"],"metadata":{"id":"IRmd9NQ_TpyT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","stat = {}\n","for model, dataset in datasets.items():\n","  print(f'Processing {model}')\n","\n","  dataset['generated'] = dataset.apply(lambda x: remove_prompts(x), axis=1)\n","  dataset['generated'] = dataset['generated'].apply(lambda x: remove_bad_chars(x))\n","  dataset['generated'] = dataset.apply(lambda x: remove_unended_sentence(x), axis=1)\n","  dataset['generated'] = dataset.progress_apply(lambda x: shorten_generated(x), axis=1)\n","  empty_generation = len(dataset[dataset.generated == ''])\n","\n","  dataset['generated_languages_fasttext'] = fasttext_detect_language(dataset)\n","  mismatched = dataset[(dataset.generated != '') & (dataset.language != dataset.generated_languages_fasttext)]\n","  mismatched = len(mismatched) / len(dataset) * 100\n","  print(f'{mismatched:0.2f}% mismatched based on FastText prediction')\n","\n","  dataset = unify_form(dataset, model)\n","\n","  shorts = len(dataset[dataset.length < 6])\n","\n","  dataset['unique_sentences'] = [unique_sentences(row) for index, row in tqdm(dataset.iterrows())]\n","  dataset['unique_words'] = [unique_words(row) for index, row in tqdm(dataset.iterrows())]\n","\n","  stat[model] = {'language_match' : (100 - mismatched), 'empty_generation' : empty_generation, 'short_texts': shorts, 'wordcount_mean' : dataset.length.mean(), 'wordcount_std' : dataset.length.std(), 'unique_sentences_mean' : dataset.unique_sentences.mean(), 'unique_sentences_std' : dataset.unique_sentences.std(), 'unique_words_mean' : dataset.unique_words.mean(), 'unique_words_std' : dataset.unique_words.std()}\n","  multitude = pd.concat([multitude, dataset], ignore_index=True, copy=False)"],"metadata":{"id":"yBIU_kIJe8w4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#are instruction-based prompts removed? it's ok\n","multitude[multitude.text.str.contains('You are a multilingual')].label.value_counts()"],"metadata":{"id":"dQXvv3rrXdlx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.options.display.float_format = \"{:,.2f}\".format\n","pd.DataFrame(stat).T"],"metadata":{"id":"qJpvr2_Dm-iX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title\n","#shorten human texts\n","def shorten_text(row, index):\n","  human = str(row.text).strip()\n","  if (human == ''):\n","    return human\n","  generated_lengths = [multitude.iloc[(index + (i+1)*8300)].length for i in range(0, len(datasets))]\n","  generated_length = np.average(generated_lengths)\n","  human_length = len(row.text.split())\n","  if (row.language == 'zh'):\n","    human_length = len(Text(human, hint_language_code=row.language).words)\n","\n","  while (((generated_length < (human_length - 5)) and (generated_length != 0)) or (human_length > 512)): #remove last sentence while more than 5 words longer or text longer than 512 words\n","    text = Text(human, hint_language_code=row.language)\n","    if (len(text.sentences) < 2): #single sentence will not be removed\n","      if (len(human.split()) > 512):\n","        return ' '.join(human.split()[:512])\n","      return human\n","    human = human.removesuffix(str(text.sentences[-1])).strip()\n","    human_length = len(human.split())\n","    if (row.language == 'zh'):\n","      human_length = len(Text(human, hint_language_code=row.language).words)\n","  return human"],"metadata":{"id":"ueChQqU15NU7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = multitude[multitude.label.str.contains('human')].copy()\n","dataset['text'] = dataset['text'].apply(lambda x: remove_bad_chars(x))\n","dataset['text'] = [shorten_text(row, index) for index, row in tqdm(dataset.iterrows(), total=len(dataset))]\n","human_language_fasttext = [detect(text=text.replace('\\n', ' '), low_memory=False)['lang'] for text in tqdm(dataset.text, total=len(dataset))]\n","dataset['generated_languages_fasttext'] = human_language_fasttext\n","mismatched = dataset[(dataset.language != dataset.generated_languages_fasttext)]\n","mismatched = len(mismatched) / len(dataset) *100\n","print(f'{mismatched:0.2f}% mismatched based on FastText prediction')\n","dataset['length'] = [len(x.split()) if (y != 'zh') or (x == '') else len(Text(x, hint_language_code=y).words) for (x, y) in zip(dataset.text, dataset.language)]\n","dataset['unique_sentences'] = [unique_sentences(row) for index, row in tqdm(dataset.iterrows(), total=len(dataset))]\n","dataset['unique_words'] = [unique_words(row) for index, row in tqdm(dataset.iterrows(), total=len(dataset))]\n","multitude.loc[multitude.label.str.contains('human'),:] = dataset"],"metadata":{"id":"e_04t8LcqMB7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multitude[multitude.label.str.contains('human')].length.describe()"],"metadata":{"id":"zqRC-XTd3Lta"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multitude[~multitude.label.str.contains('human')].length.describe()"],"metadata":{"id":"F_DfnyqrDAcM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multitude[multitude.label.str.contains('human')].unique_sentences.describe()"],"metadata":{"id":"Ku2eWXLETZRg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#number of all samples containing some duplicated sentences\n","len(multitude[(multitude.unique_sentences < 1) & (multitude.unique_sentences > 0)])"],"metadata":{"id":"g0NACCcGUW_S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stat['human'] = {'language_match' : (100 - mismatched), 'empty_generation' : 0, 'short_texts': 0, 'wordcount_mean' : dataset.length.mean(), 'wordcount_std' : dataset.length.std(), 'unique_sentences_mean' : dataset.unique_sentences.mean(), 'unique_sentences_std' : dataset.unique_sentences.std(), 'unique_words_mean' : dataset.unique_words.mean(), 'unique_words_std' : dataset.unique_words.std()}"],"metadata":{"id":"bPPvoQTRjzIa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(stat).T"],"metadata":{"id":"lI07xoADjGrK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["temp = pd.DataFrame(stat).T.reset_index()\n","temp\n","with open('generated_stat.tex', 'wt') as out: temp.to_latex(buf=out, index=False, na_rep=0, escape=False, formatters={\"text\": str.lower}, float_format=\"{:.2f}\".format)"],"metadata":{"id":"pPndIvkIyD-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#if length of human texts trimmed to 512 words\n","human_length = dataset.length.copy()\n","human_length = pd.Series([min(x, 512) for x in human_length])\n","human_length.describe()"],"metadata":{"id":"PtPvmD8QlB4t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"s3ry_O8fLjod"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#keep = [x not in to_be_changed.title.to_list() for x in multitude.title]\n","#multitude[keep].language.value_counts() / 7"],"metadata":{"id":"RxKt0XplLmXA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Language Mismatch Analysis"],"metadata":{"id":"tXQPWjA-Vgfy"}},{"cell_type":"code","source":["#human texts language mismatch\n","dataset = multitude[multitude.label.str.contains('human')]\n","mismatched = dataset[(dataset.language != dataset.generated_languages_fasttext)]"],"metadata":{"id":"uT-b2KP9hwAC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mismatched = multitude[(multitude.generated != '') & (multitude.generated_languages_fasttext.notna()) & (multitude.language != multitude.generated_languages_fasttext)]"],"metadata":{"id":"02Ucxld-qaCn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mismatched[['language']].value_counts()"],"metadata":{"id":"iJgHTNlGsdEx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mismatched[['language', 'generated_languages_fasttext']].value_counts()"],"metadata":{"id":"mH9yf0Lgqeu6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mismatched[['label']].value_counts()"],"metadata":{"id":"2GpZ1LM6shpN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mismatched[['label', 'language']].value_counts()"],"metadata":{"id":"pk6nRDi6sqzy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mismatched[mismatched.label == 'llama-65b'][['language', 'generated_languages_fasttext']].value_counts()"],"metadata":{"id":"TqOdLuuWs5aZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HN_RyKgBg_7D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset Analysis & Clearing"],"metadata":{"id":"feKTfpATXrKE"}},{"cell_type":"code","source":["multitude.head()"],"metadata":{"id":"tL18jFXItTgG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multitude.label.value_counts()"],"metadata":{"id":"AnLSk1wR2hu3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#move samples from duplicated titles that are in both splits from test split to train split\n","print(multitude.groupby(['title']).split.value_counts()['Daybreak Africa | Voice of America - English'])\n","print(multitude.groupby(['title']).split.value_counts()['International Edition 2330 EDT'])\n","print(multitude.groupby(['title']).split.value_counts()['Voice of America - English'])\n","\n","#delete from test split\n","#multitude = multitude[((multitude.title != 'Daybreak Africa | Voice of America - English') & (multitude.title != 'International Edition 2330 EDT') & (multitude.title != 'Voice of America - English')) | (multitude.split != 'test')]\n","\n","#just move to train split\n","multitude.loc[((multitude.title == 'Daybreak Africa | Voice of America - English') | (multitude.title == 'International Edition 2330 EDT') | (multitude.title == 'Voice of America - English')), 'split'] = \"train\"\n","\n","print(multitude.groupby(['title']).split.value_counts()['Daybreak Africa | Voice of America - English'])\n","print(multitude.groupby(['title']).split.value_counts()['International Edition 2330 EDT'])\n","print(multitude.groupby(['title']).split.value_counts()['Voice of America - English'])"],"metadata":{"id":"6-IFlDhStfyh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#delete empty and too-short (less than 6 words) texts\n","multitude.drop(columns=['url', 'title', 'generated', 'generated_languages_fasttext', 'unique_sentences',\t'unique_words'], inplace=True)\n","multitude.loc[multitude.text == \"nan\", \"text\"] = pd.NA\n","multitude.loc[multitude.text == \"\", \"text\"] = pd.NA\n","multitude.dropna(inplace=True)\n","multitude = multitude[multitude.length > 5]"],"metadata":{"id":"7zJHTnv18PcQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multitude.label.value_counts()"],"metadata":{"id":"kPi1t2r1YPXJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#delete text duplicates\n","multitude = multitude.drop_duplicates(subset=['text'])"],"metadata":{"id":"6SjLHHzzcTrv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multitude.label.value_counts()"],"metadata":{"id":"eK7i_N5ns-yI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multitude.language.value_counts()"],"metadata":{"id":"Z9LpTXCPJt_0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multitude.groupby(['label'])['language'].value_counts()"],"metadata":{"id":"AuFuIbqPZa49"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multitude.split.value_counts()"],"metadata":{"id":"yOoT4DpGZbit"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multitude.groupby(['split'])['language'].value_counts()"],"metadata":{"id":"vrwEkbU_p07K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multitude['multi_label'] = multitude['label'].copy()\n","multitude['label'] = int(0)\n","multitude.loc[~multitude.multi_label.str.contains('human'), 'label'] = int(1)\n","multitude = multitude.sample(frac=1., random_state = 0).reset_index(drop=True)\n","multitude.to_csv(datapath + f'multitude.csv', index=False)"],"metadata":{"id":"VdKY2DYXxVoZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multitude.head()"],"metadata":{"id":"rrJJsPD20c11"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(multitude[multitude.multi_label.str.contains('human')].length.describe())\n","print(multitude[~multitude.multi_label.str.contains('human')].length.describe())"],"metadata":{"id":"Clr4jfR_Tblq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","pd.set_option('display.max_rows', 100)\n","multitude = pd.read_csv(datapath + f'multitude.csv')"],"metadata":{"id":"hRRKdVmjBbOv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multitude.head()"],"metadata":{"id":"P7ocnc3MBo3_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#are instruction-based prompts removed? it's ok\n","phrases = ['You are a multilingual', 'Task:', 'Instruction:', 'Response:', 'Output:']\n","for phrase in phrases:\n","  print(f'{phrase}\\n{multitude[multitude.text.str.contains(phrase)].multi_label.value_counts()}\\n')"],"metadata":{"id":"53f8vAXno7R1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multitude[multitude.split=='train'].groupby(['multi_label']).language.value_counts()"],"metadata":{"id":"UBmCVrByCE2Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multitude[multitude.split=='test'].groupby(['multi_label']).language.value_counts()"],"metadata":{"id":"jmrFhs5ZCXS0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install language_data > /dev/null\n","from langcodes import *\n","#multitude[multitude.split=='test'].language.value_counts().reset_index().sort_values(by=['index']).to_latex(index=False, formatters={\"multi_label\": str.lower}, float_format=\"{:.1f}\".format)\n","temp_train = multitude[multitude.split=='train'].groupby('language')['text'].count().reset_index()\n","temp_test = multitude[multitude.split=='test'].groupby('language')['text'].count().reset_index()\n","temp = temp_train.merge(temp_test, how='outer', on=['language']).sort_values(by=['language'])\n","temp['language'] = [Language.make(language=x).display_name() for x in temp['language']]\n","temp = temp.sort_values(by=['language'])\n","temp = temp.rename(columns={'language':'Language', 'text_x':'Train', 'text_y':'Test'})\n","total = temp.sum()\n","total['Language'] = '\\textbf{Total}'\n","temp = temp.append(total,ignore_index=True)\n","temp\n","with open('table.tex', 'wt') as out: temp.to_latex(buf=out, index=False, na_rep=0, escape=False, formatters={\"text\": str.lower}, float_format=\"{:.0f}\".format)"],"metadata":{"id":"yUYgFTGSE9xA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["temp_train = multitude[multitude.split=='train'].groupby('multi_label')['text'].count().reset_index()\n","temp_test = multitude[multitude.split=='test'].groupby('multi_label')['text'].count().reset_index()\n","temp = temp_train.merge(temp_test, how='outer', on=['multi_label']).sort_values(by=['multi_label'])\n","temp = temp.rename(columns={'multi_label':'Generator', 'text_x':'Train', 'text_y':'Test'})\n","human = temp[temp.Generator.str.contains('human')]\n","machine = temp[~temp.Generator.str.contains('human')]\n","#temp = pd.concat([human, machine], ignore_index=True)\n","temp = machine\n","total = temp.sum()\n","total['Generator'] = '\\textbf{machine}'\n","temp = temp.append(total,ignore_index=True)\n","human['Generator'] = '\\textbf{human}'\n","temp = temp.append(human,ignore_index=True)\n","temp\n","with open('table.tex', 'wt') as out: temp.to_latex(buf=out, index=False, na_rep=0, escape=False, formatters={\"text\": str.lower}, float_format=\"{:.0f}\".format)"],"metadata":{"id":"PP9HA_JojonY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multitude[multitude.split=='train'].groupby('language')['text'].count()"],"metadata":{"id":"2eq_6e1hM9Ow"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Additional Human data\n","If dataset balancing required"],"metadata":{"id":"WFwHDo0Lq1j5"}},{"cell_type":"code","source":["import pandas as pd\n","pd.set_option('display.max_rows', 100)\n","multitude = pd.read_csv(datapath + f'multitude.csv')"],"metadata":{"id":"0Yef9XHirMDk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multitude.head()"],"metadata":{"id":"kfA1dEJ6rcu4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(datapath + f'MassiveSumm_selected2.csv')"],"metadata":{"id":"PPkjY8WUq7II"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.language.value_counts()"],"metadata":{"id":"HZXKiXyUq7Ta"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.drop(columns=['url', 'title'], inplace=True)"],"metadata":{"id":"U2CV5FwWq7WT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#provide missing number of human samples in multitude dataset\n","temp = multitude.groupby(['language'])[['label']].value_counts().reset_index()\n","temp.loc[temp.label == 0, 0] *= -1\n","temp = temp.groupby('language')[0].sum().reset_index()\n","print(temp)\n","df_selected = pd.DataFrame()\n","for idx, row in temp.iterrows():\n","  df_selected = pd.concat([df_selected, df[df.language == row.language].sample(min(row[0], len(df[df.language == row.language])), random_state = 0)])\n","df_selected.language.value_counts()"],"metadata":{"id":"CspEA_-IxpmD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['multi_label'] = 'human'\n","df['label'] = 0"],"metadata":{"id":"lZtB6RqVq7Y6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#shorten human texts to <512 words\n","def shorten_human_text(row, index):\n","  human = str(row.text).strip()\n","  if (human == ''):\n","    return human\n","  human_length = len(row.text.split())\n","  if (row.language == 'zh'):\n","    human_length = len(Text(human, hint_language_code=row.language).words)\n","\n","  while (human_length > 512): #remove last sentence while text longer than 512 words\n","    text = Text(human, hint_language_code=row.language)\n","    if (len(text.sentences) < 2): #single sentence will not be removed\n","      if (len(human.split()) > 512):\n","        return ' '.join(human.split()[:512])\n","      return human\n","    human = human.removesuffix(str(text.sentences[-1])).strip()\n","    human_length = len(human.split())\n","    if (row.language == 'zh'):\n","      human_length = len(Text(human, hint_language_code=row.language).words)\n","  return human"],"metadata":{"id":"vkuO7AKNq7en"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['text'] = df['text'].apply(lambda x: remove_bad_chars(x))\n","df['text'] = [shorten_human_text(row, index) for index, row in tqdm(df.iterrows(), total=len(df))]\n","df['length'] = [len(x.split()) if (y != 'zh') or (x == '') else len(Text(x, hint_language_code=y).words) for (x, y) in zip(df.text, df.language)]"],"metadata":{"id":"i6wTIM-lvh5e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multitude2 = pd.concat([multitude, df])\n","multitude2 = multitude2.drop_duplicates(subset=['text'])\n","print(multitude2.label.value_counts())\n","print(multitude2.language.value_counts())\n","print(multitude2.multi_label.value_counts())"],"metadata":{"id":"kN_JuEMJNrL3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multitude2 = multitude2.sample(frac=1., random_state = 0).reset_index(drop=True)\n","multitude2.to_csv(datapath + f'MULTITuDE2.csv', index=False)"],"metadata":{"id":"jk1fnx8UOW0G"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["7pbJVlXir8CX","YelspfIzMpIO","q3elxDVUMu5d","S8I-Pf8XPG3E","dozsOzyyshBd","HxOn9uQmt3r0","EZDAEgtE-He7","2q7GoG7OPfFE","WFwHDo0Lq1j5"],"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPoEbeFeGwmi6rFFrig3VIA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}